{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b88b6fb",
   "metadata": {},
   "source": [
    "# **Lunar Lander DQN Simulation**\n",
    "\n",
    "This simulation utilizes Deep Q-Network (DQN) to train an agent to land a spacecraft on the lunar surface. The DQN is a type of artificial intelligence model that combines deep neural networks and Q-learning, a type of reinforcement learning, to optimize decision-making processes in environments with discrete actions.\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "The simulation uses the `LunarLander-v2` environment from the `gym` library, a toolkit for developing and comparing reinforcement learning algorithms. The aim is to guide a lunar lander to a safe landing on a landing pad. The environment provides a state containing positional and velocity information about the lander and returns rewards based on the lander's actions. The DQN agent learns from these rewards to improve its landing strategy over time.\n",
    "\n",
    "The user interface for the simulation is created using the `pygame` library, providing a visual representation of the lunar lander's position, angle, and landing success rate.\n",
    "\n",
    "## **Key Components**\n",
    "\n",
    "1. **Environment Initialization:** The `LunarLander-v2` environment from `gym` is initialized, providing the framework for the DQN agent's interactions.\n",
    "  \n",
    "2. **Neural Network Model:** The DQN consists of a neural network model, implemented using `tensorflow` and `keras`. This model takes in the state of the lunar lander and outputs the best action to take.\n",
    "\n",
    "3. **Experience Replay:** As the agent interacts with the environment, it stores its experiences in a memory buffer. Periodically, the agent samples from this buffer to train its neural network, improving its decision-making abilities.\n",
    "\n",
    "4. **Exploration vs. Exploitation:** The agent employs an epsilon-greedy strategy, where it occasionally takes a random action (exploration) but predominantly chooses the action recommended by its neural network (exploitation). Over time, the agent reduces its exploration rate.\n",
    "\n",
    "5. **Visual Simulation with Pygame:** The position, angle, and landing success rate of the lunar lander are visualized using the `pygame` library. This provides an interactive display, allowing users to watch the agent's progress in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## **Getting Started:**\n",
    "\n",
    "**Install the required packages**\n",
    "\n",
    "These packages facilitate various functionalities, ranging from the reinforcement learning environment, deep learning model creation, to the interactive visualization:\n",
    "\n",
    "- **Gym**  # Reinforcement learning environment\n",
    "- **Pygame**   # Game development and visualization\n",
    "- **Numpy**  # Numerical computations\n",
    "- **Tensorflow**  # Deep learning framework\n",
    "- **Keras**  # High-level neural networks API\n",
    "- **matplotlib** # Used to create a display graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ff80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "## -- pip install gym box2d-py tensorflow pygame -- ##\n",
    "import gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress all but error logs\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # Suppress TensorFlow info and warning logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fe6d2",
   "metadata": {},
   "source": [
    "**Pygame Setup and Display Configuration**\n",
    "\n",
    "This section initializes the Pygame library, which is essential for creating interactive games and simulations. It then defines the display settings, setting the screen's width and height to 600x400 pixels. A window is created using these dimensions, and it's titled \"Lunar Lander DQN Simulation.\" For visualization, background and rocket images are loaded and resized to fit appropriately on the screen. Lastly, standard colors (WHITE and GREEN) and a font for displaying iteration numbers are defined for further use in the simulation's user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "\n",
    "# Display settings\n",
    "SCREEN_WIDTH = 600\n",
    "SCREEN_HEIGHT = 400\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Lunar Lander DQN Simulation\")\n",
    "\n",
    "# Load and resize images\n",
    "background_image = pygame.transform.scale(pygame.image.load(\"moon.png\"), (SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "rocket_image = pygame.transform.scale(pygame.image.load(\"rocket.png\"), (150, 150))\n",
    "\n",
    "# Define colors and fonts\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "iteration_font = pygame.font.Font(None, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9c71a",
   "metadata": {},
   "source": [
    "**User Interface Rendering for Lunar Lander Simulation**\n",
    "\n",
    "This set of functions is dedicated to rendering the visual components of the Lunar Lander DQN Simulation. The `draw_lander` function takes the x and y coordinates, along with the angle of the rocket, to draw a rotated rocket image on the screen. In contrast, the `draw_ui_text` function displays the current episode number and the success rate of landings to keep the user informed about the simulation's progress. The `draw_landing_pad` function visualizes the landing pad; its color indicates whether the landing is successful (GREEN) or not (WHITE). Finally, the `draw_lunar_ui` function integrates all these components to create the complete user interface for the simulation, showcasing the lander, the landing pad, the episode information, and the success rate on a lunar background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70158117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lander(x, y, angle):\n",
    "    \"\"\"Draw the rocket on the screen with a given position and angle.\"\"\"\n",
    "    screen_x = int((x + 1) * SCREEN_WIDTH / 2)\n",
    "    screen_y = int(SCREEN_HEIGHT - y * SCREEN_HEIGHT / 2)\n",
    "    rotated_rocket = pygame.transform.rotate(rocket_image, -np.degrees(angle))\n",
    "    rocket_rect = rotated_rocket.get_rect(center=(screen_x, screen_y))\n",
    "    screen.blit(rotated_rocket, rocket_rect.topleft)\n",
    "\n",
    "def draw_ui_text(episodeIndex, success_percentage):\n",
    "    \"\"\"Draw episode and success rate text on the screen.\"\"\"\n",
    "    episode_text = iteration_font.render(f\"Episode: {episodeIndex+1}\", True, WHITE)\n",
    "    success_text = iteration_font.render(f\"Success Rate: {success_percentage:.2f}%\", True, WHITE)\n",
    "    screen.blit(episode_text, (int(SCREEN_WIDTH * 0.07), int(SCREEN_HEIGHT * 0.9)))\n",
    "    screen.blit(success_text, (int(SCREEN_WIDTH * 0.7), int(SCREEN_HEIGHT * 0.9)))\n",
    "\n",
    "def draw_landing_pad(successful_landing):\n",
    "    \"\"\"Draw the landing pad on the screen.\"\"\"\n",
    "    pad_width = 100\n",
    "    pad_height = 15\n",
    "    pad_x = (SCREEN_WIDTH - pad_width) // 2\n",
    "    pad_y = SCREEN_HEIGHT - pad_height\n",
    "    color = GREEN if successful_landing else WHITE\n",
    "    pygame.draw.rect(screen, color, (pad_x, pad_y, pad_width, pad_height))\n",
    "\n",
    "def draw_lunar_ui(x, y, angle, episodeIndex, success_percentage, successful_landing):\n",
    "    \"\"\"Draw the entire user interface for the lunar lander simulation.\"\"\"\n",
    "    screen.blit(background_image, (0, 0))\n",
    "    draw_lander(x, y, angle)\n",
    "    draw_ui_text(episodeIndex, success_percentage)\n",
    "    draw_landing_pad(successful_landing)\n",
    "    pygame.display.flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceefcf7",
   "metadata": {},
   "source": [
    "**Deep Q-Network (DQN) for Lunar Lander Training**\n",
    "\n",
    "The `DQN` class is designed to implement the Deep Q-Network algorithm, a type of reinforcement learning, specifically tailored for training a lunar lander agent. The class contains several key components:\n",
    "\n",
    "- **Initialization (`__init__`)**: Upon instantiation, the class initializes the environment, defines the state and action sizes, and sets various parameters such as the memory buffer size (`memory`), discount factor (`gamma`), exploration rate (`epsilon`), and learning rate (`learning_rate`). It also constructs two neural network models: the primary model (`model`) and the target model (`target_model`).\n",
    "\n",
    "- **Neural Network Model (`_build_model`)**: This method establishes the architecture of the neural network model. It consists of two hidden layers with 24 neurons each and uses the ReLU activation function. The output layer corresponds to the possible actions, and the model is compiled with Mean Squared Error (MSE) as the loss function and the Adam optimizer.\n",
    "\n",
    "- **Update Target Model (`update_target_model`)**: Ensures that the target model's weights are synchronized with the main model's weights, which is vital for stability in learning.\n",
    "\n",
    "- **Storing Experience (`remember`)**: This function allows the agent to store experiences, which are tuples of state, action, reward, next state, and a 'done' flag, into its memory. This memory is then used for training.\n",
    "\n",
    "- **Action Selection (`act`)**: Determines the agent's action. It either exploits the current knowledge (choosing the best action based on the model's predictions) or explores a new action (chooses randomly) based on the exploration rate (`epsilon`).\n",
    "\n",
    "- **Training from Memory (`replay`)**: This method trains the neural network model using randomly sampled experiences from memory. It updates the Q-values based on the reward and the highest Q-value of the next state, using the target model for predictions.\n",
    "\n",
    "- **Loading and Saving Models (`load` and `save`)**: These methods enable the user to load pre-trained weights into the model or save the current model's weights for future use.\n",
    "\n",
    "Overall, he `DQN` class provides the machinery for an agent to learn optimal strategies for landing the lunar module through trial and error, leveraging neural networks to approximate the best actions given different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eedd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"Deep Q-Network implementation for training the lunar lander.\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the neural network model for the DQN.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update the target model with weights from the main model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store the experience in memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using the DQN or a random action.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Train the DQN using experiences from the memory.\"\"\"\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([experience[0][0] for experience in minibatch])\n",
    "        next_states = np.array([experience[3][0] for experience in minibatch])\n",
    "        targets = self.model.predict(states)\n",
    "        next_state_targets = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            if done:\n",
    "                targets[i][action] = reward\n",
    "            else:\n",
    "                targets[i][action] = reward + self.gamma * np.amax(next_state_targets[i])\n",
    "                \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b17bc",
   "metadata": {},
   "source": [
    "**Setting Up the Environment and Agent Initialization**\n",
    "\n",
    "In this segment, the Lunar Lander environment is initialized using the `gym` library, which provides a standardized interface for reinforcement learning tasks. An instance of the DQN agent is then created, taking this environment as an argument, allowing it to interact and learn from it. Additionally, a batch size of 32 is specified for experience replay, indicating that 32 experiences will be randomly sampled from memory for each training iteration. The simulation is set to run for 1,000 episodes, and a counter (`successful_landings`) is initialized to zero to keep track of the number of times the agent successfully lands the lunar lander during these episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c07351",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "agent = DQN(env)\n",
    "batch_size = 32\n",
    "num_episodes = 1000\n",
    "successful_landings = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061edfde",
   "metadata": {},
   "source": [
    "**Training Loop for Lunar Lander Using Deep Q-Network (DQN)**\n",
    "\n",
    "The provided code outlines the training loop used to teach the lunar lander agent using a Deep Q-Network (DQN). Each iteration of the outer loop represents a single episode of training, and within each episode, the agent interacts with its environment for a set number of time steps.\n",
    "\n",
    "At the beginning of each episode, some initializations are made. The cumulative reward (`total_reward`) for the episode is set to zero, and the environment is reset to its initial state, providing the agent with a fresh start. This initial state is then reshaped to fit the DQN model's expected input format.\n",
    "\n",
    "The inner loop represents the agent's interactions with the environment for each time step of the episode, capped at 500 iterations. Within this loop, the agent's current state, such as its position (`x`, `y`) and angle (`angle`), is used to render the visual interface, giving real-time feedback on its performance. Based on its current state, the agent then decides on an action, either exploiting its current knowledge or exploring a new action. This action is taken in the environment, which returns the resulting new state, the reward for the action, and a flag (`done`) indicating if the episode has ended (e.g., if the lander has crashed or landed successfully).\n",
    "\n",
    "The agent's experience, consisting of its current state, chosen action, received reward, new state, and the `done` flag, is stored in its memory. This memory is used later for training the DQN. If the episode ends before the maximum number of time steps, the agent checks the total accumulated reward. If it surpasses a threshold (in this case, 50), it's deemed a successful landing, and the count of successful landings is incremented. Additionally, the DQN's target model is updated.\n",
    "\n",
    "Every 10 episodes, the DQN model is trained using a batch of experiences randomly sampled from the agent's memory, refining its understanding of beneficial actions in different states.\n",
    "\n",
    "Finally, after each episode, there's a brief pause before the next episode starts, allowing for better visualization of the agent's performance.\n",
    "\n",
    "Throughout the training process, there are checks for user-initiated exit requests, ensuring the program can be gracefully terminated at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e298347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure interactive mode is on for real-time updates\n",
    "plt.ion()\n",
    "\n",
    "# Placeholder for collecting rewards\n",
    "rewards_per_episode = []\n",
    "\n",
    "# Loop over all episodes for training\n",
    "for e in range(num_episodes):\n",
    "    # Initialize the reward for this episode\n",
    "    total_reward = 0\n",
    "    # Flag to check if the landing was successful in this episode\n",
    "    successful_landing = False\n",
    "    # Reset the environment for a new episode and get initial state\n",
    "    state, _ = env.reset()\n",
    "    # Reshape the state to match the expected input shape for the DQN model\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    \n",
    "    # Loop for each time step in the episode\n",
    "    for time in range(500):\n",
    "        # Extract position and angle information from the state\n",
    "        x = state[0][0]\n",
    "        y = state[0][1]\n",
    "        angle = state[0][4]\n",
    "        # Calculate the success percentage so far\n",
    "        success_percentage = (successful_landings / (e + 1)) * 100\n",
    "        # Draw the UI with current state information\n",
    "        draw_lunar_ui(x, y, angle, e, success_percentage, successful_landing)\n",
    "        # Get an action from the DQN agent\n",
    "        action = agent.act(state)\n",
    "        # Perform the action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "        # Modify the reward if the episode is done\n",
    "        reward = reward if not done else -10\n",
    "        # Reshape the next state to match the expected input shape for the DQN model\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        # Store this experience in the agent's memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # Set the current state for the next iteration\n",
    "        state = next_state\n",
    "\n",
    "        # Check for user exit request\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                exit()\n",
    "\n",
    "        # Check if episode is done\n",
    "        if done:\n",
    "            # If the total reward is above a threshold, consider it a successful landing\n",
    "            if total_reward > 50:\n",
    "                successful_landings += 1\n",
    "            # Update the target model of the DQN\n",
    "            agent.update_target_model()\n",
    "            break\n",
    "            \n",
    "        # Every 10 episodes, train the DQN with experiences from memory\n",
    "        if e % 10 == 0:\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "    # Collect metrics (for example, total reward per episode)\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    # Clear the current figure\n",
    "    plt.clf()\n",
    "\n",
    "    # Reinitialize the figure settings\n",
    "    plt.xlabel('Episode', fontsize=12)\n",
    "    plt.ylabel('Reward', fontsize=12)\n",
    "    plt.title('Reward per Episode over Training', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot the new data\n",
    "    plt.plot(rewards_per_episode, label=\"Reward per Episode\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Redraw the plot\n",
    "    plt.draw()\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    # Check the landing conditions at the end of an episode\n",
    "    if total_reward > 50 and y < 0.1:\n",
    "        successful_landing = True\n",
    "\n",
    "    # Wait for a short duration before starting the next episode\n",
    "    pygame.time.wait(500)\n",
    "    \n",
    "# Turn off interactive mode for stable final display\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029164bc",
   "metadata": {},
   "source": [
    "**Training Conclusion and Cleanup**\n",
    "\n",
    "This segment marks the end of the training process:\n",
    "\n",
    "- A message, \"Training completed!\", is printed to the console, signaling to the user that all episodes have been processed, and the training phase is over.\n",
    "- The `pygame.quit()` function is called to gracefully shut down the `pygame` environment, ensuring all resources are freed and no lingering processes remain. This cleanup step is crucial to maintain system stability and avoid potential issues, especially when running extensive simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64117081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed!\")\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
