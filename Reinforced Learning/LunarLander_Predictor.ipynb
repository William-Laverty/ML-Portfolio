{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b88b6fb",
   "metadata": {},
   "source": [
    "# **Lunar Lander DQN Simulation**\n",
    "\n",
    "This simulation utilizes Deep Q-Network (DQN) to train an agent to land a spacecraft on the lunar surface. The DQN is a type of artificial intelligence model that combines deep neural networks and Q-learning, a type of reinforcement learning, to optimize decision-making processes in environments with discrete actions.\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "The simulation uses the `LunarLander-v2` environment from the `gym` library, a toolkit for developing and comparing reinforcement learning algorithms. The aim is to guide a lunar lander to a safe landing on a landing pad. The environment provides a state containing positional and velocity information about the lander and returns rewards based on the lander's actions. The DQN agent learns from these rewards to improve its landing strategy over time.\n",
    "\n",
    "The user interface for the simulation is created using the `pygame` library, providing a visual representation of the lunar lander's position, angle, and landing success rate.\n",
    "\n",
    "## **Key Components**\n",
    "\n",
    "1. **Environment Initialization:** The `LunarLander-v2` environment from `gym` is initialized, providing the framework for the DQN agent's interactions.\n",
    "  \n",
    "2. **Neural Network Model:** The DQN consists of a neural network model, implemented using `tensorflow` and `keras`. This model takes in the state of the lunar lander and outputs the best action to take.\n",
    "\n",
    "3. **Experience Replay:** As the agent interacts with the environment, it stores its experiences in a memory buffer. Periodically, the agent samples from this buffer to train its neural network, improving its decision-making abilities.\n",
    "\n",
    "4. **Exploration vs. Exploitation:** The agent employs an epsilon-greedy strategy, where it occasionally takes a random action (exploration) but predominantly chooses the action recommended by its neural network (exploitation). Over time, the agent reduces its exploration rate.\n",
    "\n",
    "5. **Visual Simulation with Pygame:** The position, angle, and landing success rate of the lunar lander are visualized using the `pygame` library. This provides an interactive display, allowing users to watch the agent's progress in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## **Getting Started:**\n",
    "\n",
    "**Install the required packages**\n",
    "\n",
    "These packages facilitate various functionalities, ranging from the reinforcement learning environment, deep learning model creation, to the interactive visualization:\n",
    "\n",
    "- **Gym**  # Reinforcement learning environment\n",
    "- **Pygame**   # Game development and visualization\n",
    "- **Numpy**  # Numerical computations\n",
    "- **Tensorflow**  # Deep learning framework\n",
    "- **Keras**  # High-level neural networks API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4ff80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "## -- pip install gym box2d-py tensorflow pygame -- ##\n",
    "import gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fe6d2",
   "metadata": {},
   "source": [
    "**Pygame Setup and Display Configuration**\n",
    "\n",
    "This section initializes the Pygame library, which is essential for creating interactive games and simulations. It then defines the display settings, setting the screen's width and height to 600x400 pixels. A window is created using these dimensions, and it's titled \"Lunar Lander DQN Simulation.\" For visualization, background and rocket images are loaded and resized to fit appropriately on the screen. Lastly, standard colors (WHITE and GREEN) and a font for displaying iteration numbers are defined for further use in the simulation's user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6f162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.init()\n",
    "\n",
    "# Display settings\n",
    "SCREEN_WIDTH = 600\n",
    "SCREEN_HEIGHT = 400\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Lunar Lander DQN Simulation\")\n",
    "\n",
    "# Load and resize images\n",
    "background_image = pygame.transform.scale(pygame.image.load(\"moon.png\"), (SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "rocket_image = pygame.transform.scale(pygame.image.load(\"rocket.png\"), (150, 150))\n",
    "\n",
    "# Define colors and fonts\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "iteration_font = pygame.font.Font(None, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c9c71a",
   "metadata": {},
   "source": [
    "**User Interface Rendering for Lunar Lander Simulation**\n",
    "\n",
    "This set of functions is dedicated to rendering the visual components of the Lunar Lander DQN Simulation. The `draw_lander` function takes the x and y coordinates, along with the angle of the rocket, to draw a rotated rocket image on the screen. In contrast, the `draw_ui_text` function displays the current episode number and the success rate of landings to keep the user informed about the simulation's progress. The `draw_landing_pad` function visualizes the landing pad; its color indicates whether the landing is successful (GREEN) or not (WHITE). Finally, the `draw_lunar_ui` function integrates all these components to create the complete user interface for the simulation, showcasing the lander, the landing pad, the episode information, and the success rate on a lunar background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70158117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lander(x, y, angle):\n",
    "    \"\"\"Draw the rocket on the screen with a given position and angle.\"\"\"\n",
    "    screen_x = int((x + 1) * SCREEN_WIDTH / 2)\n",
    "    screen_y = int(SCREEN_HEIGHT - y * SCREEN_HEIGHT / 2)\n",
    "    rotated_rocket = pygame.transform.rotate(rocket_image, -np.degrees(angle))\n",
    "    rocket_rect = rotated_rocket.get_rect(center=(screen_x, screen_y))\n",
    "    screen.blit(rotated_rocket, rocket_rect.topleft)\n",
    "\n",
    "def draw_ui_text(episodeIndex, success_percentage):\n",
    "    \"\"\"Draw episode and success rate text on the screen.\"\"\"\n",
    "    episode_text = iteration_font.render(f\"Episode: {episodeIndex+1}\", True, WHITE)\n",
    "    success_text = iteration_font.render(f\"Success Rate: {success_percentage:.2f}%\", True, WHITE)\n",
    "    screen.blit(episode_text, (int(SCREEN_WIDTH * 0.07), int(SCREEN_HEIGHT * 0.9)))\n",
    "    screen.blit(success_text, (int(SCREEN_WIDTH * 0.7), int(SCREEN_HEIGHT * 0.9)))\n",
    "\n",
    "def draw_landing_pad(successful_landing):\n",
    "    \"\"\"Draw the landing pad on the screen.\"\"\"\n",
    "    pad_width = 100\n",
    "    pad_height = 15\n",
    "    pad_x = (SCREEN_WIDTH - pad_width) // 2\n",
    "    pad_y = SCREEN_HEIGHT - pad_height\n",
    "    color = GREEN if successful_landing else WHITE\n",
    "    pygame.draw.rect(screen, color, (pad_x, pad_y, pad_width, pad_height))\n",
    "\n",
    "def draw_lunar_ui(x, y, angle, episodeIndex, success_percentage, successful_landing):\n",
    "    \"\"\"Draw the entire user interface for the lunar lander simulation.\"\"\"\n",
    "    screen.blit(background_image, (0, 0))\n",
    "    draw_lander(x, y, angle)\n",
    "    draw_ui_text(episodeIndex, success_percentage)\n",
    "    draw_landing_pad(successful_landing)\n",
    "    pygame.display.flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceefcf7",
   "metadata": {},
   "source": [
    "**Deep Q-Network (DQN) Class Initialization**\n",
    "\n",
    "This `DQN` class encapsulates the Deep Q-Network implementation tailored for the lunar lander's training. Upon initialization, the class sets up the environment, determining the size of the state and possible actions from the provided environment. A memory buffer is initialized with a maximum length of 2000 to store experiences for training. Several parameters, crucial for the Q-learning algorithm, are set, such as the discount factor (`gamma`), exploration rate (`epsilon`), its minimum value (`epsilon_min`), and its decay rate (`epsilon_decay`). The learning rate for the neural network training is also specified. The class then constructs two neural network models: the main model (`model`) used for decision-making and a target model (`target_model`) that aids in stabilizing the learning process. The target model's weights are initialized to match the main model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1eedd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\"Deep Q-Network implementation for training the lunar lander.\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1752460",
   "metadata": {},
   "source": [
    "**Neural Network Construction for DQN**\n",
    "\n",
    "The `_build_model` function constructs the neural network architecture that underpins the Deep Q-Network (DQN). The model is built using a sequential arrangement of layers. The first layer consists of 24 neurons with a ReLU activation function, taking the state's size as its input dimension. This is followed by another dense layer with 24 neurons and a ReLU activation function. The final layer has a neuron count equal to the number of possible actions and uses a linear activation function, outputting the Q-values for each action. The model is compiled using the Mean Squared Error (MSE) as its loss function and the Adam optimizer, with the specified learning rate, to adjust the network's weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd61cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _build_model(self):\n",
    "        \"\"\"Build the neural network model for the DQN.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1a11d",
   "metadata": {},
   "source": [
    "**DQN Model Operations and Decision-Making**\n",
    "\n",
    "Within these methods, core operations of the DQN are defined:\n",
    "\n",
    "- `update_target_model`: This function synchronizes the weights of the target model with those of the primary DQN model. This step is essential in Q-learning with neural networks to stabilize the learning process by having a fixed target for Q-value estimations.\n",
    "  \n",
    "- `remember`: This function captures and stores an experience in the agent's memory. An experience, in this context, comprises the state, the action taken, the reward received, the next state, and a flag indicating if the episode ended. By storing these experiences, the agent can later sample from them to train its neural network, a process known as experience replay.\n",
    "\n",
    "- `act`: This function determines the action the agent should take. It uses an epsilon-greedy approach: with a probability of `epsilon`, it chooses a random action (exploration), and with a probability of \\(1 - \\epsilon\\), it queries the DQN model to select the action with the highest predicted Q-value (exploitation). This blend of exploration and exploitation ensures the agent can discover new strategies while also capitalizing on what it has already learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a02dfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_target_model(self):\n",
    "        \"\"\"Update the target model with weights from the main model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store the experience in memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using the DQN or a random action.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fd0e2",
   "metadata": {},
   "source": [
    "**Experience Replay and DQN Training**\n",
    "\n",
    "The `replay` function facilitates the training of the DQN model using a concept known as experience replay. By randomly sampling a batch of experiences from the agent's memory, the function breaks correlations between consecutive experiences, enhancing the stability of the training process. For each experience in the minibatch:\n",
    "\n",
    "1. The current and next states are passed through the DQN and the target model, respectively, to obtain predicted Q-values.\n",
    "\n",
    "2. For the given action, if the episode ended (`done` is True), the target Q-value is simply the received reward. If the episode continued, the target Q-value is computed as the sum of the received reward and the discounted maximum Q-value predicted for the next state by the target model.\n",
    "\n",
    "3. With these target Q-values and the original states, the DQN model is trained for one epoch, adjusting its weights to minimize the difference between its predictions and the computed target Q-values.\n",
    "\n",
    "4. Lastly, the exploration rate (`epsilon`) decays by a factor (`epsilon_decay`), promoting a gradual shift from exploration to exploitation as the agent learns more about the environment. However, the decay stops once `epsilon` reaches a minimum threshold (`epsilon_min`), ensuring some level of exploration is always maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96fae924",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def replay(self, batch_size):\n",
    "        \"\"\"Train the DQN using experiences from the memory.\"\"\"\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([experience[0][0] for experience in minibatch])\n",
    "        next_states = np.array([experience[3][0] for experience in minibatch])\n",
    "        targets = self.model.predict(states)\n",
    "        next_state_targets = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            if done:\n",
    "                targets[i][action] = reward\n",
    "            else:\n",
    "                targets[i][action] = reward + self.gamma * np.amax(next_state_targets[i])\n",
    "                \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b07d3",
   "metadata": {},
   "source": [
    "**Model Persistence: Saving and Loading Weights**\n",
    "\n",
    "These functions, `load` and `save`, provide the capability for model persistence. The `load` function allows for the restoration of previously trained weights into the DQN model, facilitating the continuation of training or policy deployment without starting from scratch. Conversely, the `save` function offers the ability to store the current weights of the DQN model, ensuring that the knowledge acquired by the agent during its training sessions can be saved and reused at a later time. This ability is crucial for scenarios where training is time-consuming, or the learned policy needs to be transferred or deployed elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e28cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b17bc",
   "metadata": {},
   "source": [
    "**Setting Up the Environment and Agent Initialization**\n",
    "\n",
    "In this segment, the Lunar Lander environment is initialized using the `gym` library, which provides a standardized interface for reinforcement learning tasks. An instance of the DQN agent is then created, taking this environment as an argument, allowing it to interact and learn from it. Additionally, a batch size of 32 is specified for experience replay, indicating that 32 experiences will be randomly sampled from memory for each training iteration. The simulation is set to run for 1,000 episodes, and a counter (`successful_landings`) is initialized to zero to keep track of the number of times the agent successfully lands the lunar lander during these episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c07351",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQN' object has no attribute '_build_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mLunarLander-v2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m agent \u001b[39m=\u001b[39m DQN(env)\n\u001b[1;32m      3\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      4\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_decay \u001b[39m=\u001b[39m \u001b[39m0.995\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_model()\n\u001b[1;32m     15\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_model()\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_target_model()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute '_build_model'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "agent = DQN(env)\n",
    "batch_size = 32\n",
    "num_episodes = 1000\n",
    "successful_landings = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061edfde",
   "metadata": {},
   "source": [
    "**Training Loop Initialization**\n",
    "\n",
    "This code initiates the main training loop for the agent over a specified number of episodes. For each episode:\n",
    "\n",
    "- The cumulative reward, represented by `total_reward`, is initialized to zero. This will accumulate the rewards the agent receives during the episode, providing a measure of the agent's performance.\n",
    "\n",
    "- A boolean flag, `successful_landing`, is set to `False` by default. This flag will be used later to determine if the agent achieved a successful landing in the current episode.\n",
    "\n",
    "- The environment is reset using the `env.reset()` method, signifying the start of a new episode. This method returns the initial state of the environment.\n",
    "\n",
    "- The obtained state is then reshaped to fit the expected input shape of the DQN model. This ensures compatibility when the state is passed to the neural network for action prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e298347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all episodes for training\n",
    "for e in range(num_episodes):\n",
    "    # Initialize the reward for this episode\n",
    "    total_reward = 0\n",
    "    # Flag to check if the landing was successful in this episode\n",
    "    successful_landing = False\n",
    "    # Reset the environment for a new episode and get initial state\n",
    "    state, _ = env.reset()\n",
    "    # Reshape the state to match the expected input shape for the DQN model\n",
    "    state = np.reshape(state, [1, agent.state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ba275",
   "metadata": {},
   "source": [
    "**Episode Time-Step Loop**\n",
    "\n",
    "This code segment introduces a loop that processes each time step within an episode, with a maximum of 500 time steps per episode. For every time step:\n",
    "\n",
    "- Positional and angular data are extracted from the current state of the environment. Specifically, `x` and `y` represent the horizontal and vertical positions of the lunar lander, respectively, while `angle` denotes its orientation. These extracted values can be used for visualization or to influence the agent's decision-making process during the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ebc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Loop for each time step in the episode\n",
    "    for time in range(500):\n",
    "        # Extract position and angle information from the state\n",
    "        x = state[0][0]\n",
    "        y = state[0][1]\n",
    "        angle = state[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c118d85",
   "metadata": {},
   "source": [
    "**Time-Step Processing and Interaction with Environment**\n",
    "\n",
    "In this section, the agent interacts with the environment at each time step, making decisions and receiving feedback:\n",
    "\n",
    "- The success percentage is calculated by dividing the number of successful landings by the total episodes so far. This gives a running measure of the agent's performance.\n",
    "\n",
    "- The `draw_lunar_ui` function is called to visualize the lunar lander's current state, including its position, angle, episode number, and success percentage.\n",
    "\n",
    "- The agent then decides on an action based on the current state by calling the `act` method. \n",
    "\n",
    "- This chosen action is executed in the environment using the `env.step(action)` method, which returns the next state, the reward for the action, a flag indicating if the episode has ended (`done`), and other information.\n",
    "\n",
    "- The received reward is added to the cumulative `total_reward` for the episode. If the episode ends prematurely (e.g., if the lander crashes), the reward is modified to a negative value to penalize the agent.\n",
    "\n",
    "- The next state is reshaped to fit the input format expected by the DQN model.\n",
    "\n",
    "- The experience, which comprises the current state, action taken, received reward, next state, and the `done` flag, is stored in the agent's memory using the `remember` method. This memory will be used later for experience replay, a technique to improve the stability of DQN training.\n",
    "\n",
    "- The next state becomes the current state for the subsequent time step, preparing the agent for the next iteration of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733372f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Calculate the success percentage so far\n",
    "        success_percentage = (successful_landings / (e + 1)) * 100\n",
    "        # Draw the UI with current state information\n",
    "        draw_lunar_ui(x, y, angle, e, success_percentage, successful_landing)\n",
    "        # Get an action from the DQN agent\n",
    "        action = agent.act(state)\n",
    "        # Perform the action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "        # Modify the reward if the episode is done\n",
    "        reward = reward if not done else -10\n",
    "        # Reshape the next state to match the expected input shape for the DQN model\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        # Store this experience in the agent's memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # Set the current state for the next iteration\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebdf61c",
   "metadata": {},
   "source": [
    "**User Interaction and Exit Handling**\n",
    "\n",
    "This segment of the code handles user interactions within the `pygame` window. Specifically, it listens for events in the pygame window, and if a user requests to close the window or exit the simulation (e.g., by clicking the close button), the code responds by gracefully shutting down the `pygame` environment and then exiting the program entirely. This ensures a smooth user experience by allowing the user to terminate the simulation at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5079bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Check for user exit request\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acea8cc",
   "metadata": {},
   "source": [
    "**Episode Termination and Performance Evaluation**\n",
    "\n",
    "This section addresses the scenario when an episode concludes, either due to the agent achieving its objective or failing in its task. \n",
    "\n",
    "- The `done` flag indicates whether the episode has ended. If it's set to `True`, further checks are conducted to evaluate the agent's performance.\n",
    "\n",
    "- The agent's cumulative reward (`total_reward`) for the episode is examined. If it surpasses a threshold of 50, it implies that the agent has successfully landed the lunar lander, and thus the count of `successful_landings` is incremented.\n",
    "\n",
    "- Subsequently, the target model of the DQN is updated to synchronize with the weights of the main model using the `update_target_model` method. This step is crucial to maintain stability during the Q-learning process.\n",
    "\n",
    "- The `break` statement then exits the time-step loop prematurely, marking the end of the current episode and paving the way for the next episode to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Check if episode is done\n",
    "        if done:\n",
    "            # If the total reward is above a threshold, consider it a successful landing\n",
    "            if total_reward > 50:\n",
    "                successful_landings += 1\n",
    "            # Update the target model of the DQN\n",
    "            agent.update_target_model()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d163df",
   "metadata": {},
   "source": [
    "**Periodic Training, Landing Evaluation, and Inter-Episode Delay**\n",
    "\n",
    "This section of the code encompasses several key actions that occur during and after each episode:\n",
    "\n",
    "- The agent's DQN model undergoes training every 10 episodes. This is achieved by checking if the current episode number (`e`) is a multiple of 10. If true, and the length of stored experiences in the agent's memory exceeds the defined batch size, the `replay` method is called. This method trains the DQN model using randomly sampled experiences from the agent's memory, leveraging the technique of experience replay.\n",
    "\n",
    "- After all the time-steps of an episode are processed, the agent evaluates whether it achieved a successful landing. The criteria for success are that the cumulative reward (`total_reward`) for the episode exceeds 50 and the vertical position (`y`) of the lander is less than 0.1. If both conditions are met, the `successful_landing` flag is set to `True`.\n",
    "\n",
    "- Finally, before transitioning to the next episode, there's a brief pause (or wait time) of 500 milliseconds. This delay, implemented using `pygame.time.wait(500)`, provides a short break, allowing users to visually process the outcome of the current episode before the next one commences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ed9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Every 10 episodes, train the DQN with experiences from memory\n",
    "        if e % 10 == 0:\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "    # Check the landing conditions at the end of an episode\n",
    "    if total_reward > 50 and y < 0.1:\n",
    "        successful_landing = True\n",
    "\n",
    "    # Wait for a short duration before starting the next episode\n",
    "    pygame.time.wait(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029164bc",
   "metadata": {},
   "source": [
    "**Training Conclusion and Cleanup**\n",
    "\n",
    "This segment marks the end of the training process:\n",
    "\n",
    "- A message, \"Training completed!\", is printed to the console, signaling to the user that all episodes have been processed, and the training phase is over.\n",
    "- The `pygame.quit()` function is called to gracefully shut down the `pygame` environment, ensuring all resources are freed and no lingering processes remain. This cleanup step is crucial to maintain system stability and avoid potential issues, especially when running extensive simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64117081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed!\")\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
