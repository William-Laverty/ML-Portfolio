{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ff80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym box2d-py tensorflow pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64117081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "# --- Pygame Initialization ---\n",
    "pygame.init()\n",
    "\n",
    "# Display settings\n",
    "SCREEN_WIDTH = 600\n",
    "SCREEN_HEIGHT = 400\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Lunar Lander DQN Simulation\")\n",
    "\n",
    "# Load and resize images\n",
    "background_image = pygame.transform.scale(pygame.image.load(\"moon.png\"), (SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "rocket_image = pygame.transform.scale(pygame.image.load(\"rocket.png\"), (150, 150))\n",
    "\n",
    "# Define colors and fonts\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "iteration_font = pygame.font.Font(None, 24)\n",
    "\n",
    "def draw_lander(x, y, angle):\n",
    "    \"\"\"Draw the rocket on the screen with a given position and angle.\"\"\"\n",
    "    screen_x = int((x + 1) * SCREEN_WIDTH / 2)\n",
    "    screen_y = int(SCREEN_HEIGHT - y * SCREEN_HEIGHT / 2)\n",
    "    rotated_rocket = pygame.transform.rotate(rocket_image, -np.degrees(angle))\n",
    "    rocket_rect = rotated_rocket.get_rect(center=(screen_x, screen_y))\n",
    "    screen.blit(rotated_rocket, rocket_rect.topleft)\n",
    "\n",
    "def draw_ui_text(episodeIndex, success_percentage):\n",
    "    \"\"\"Draw episode and success rate text on the screen.\"\"\"\n",
    "    episode_text = iteration_font.render(f\"Episode: {episodeIndex+1}\", True, WHITE)\n",
    "    success_text = iteration_font.render(f\"Success Rate: {success_percentage:.2f}%\", True, WHITE)\n",
    "    screen.blit(episode_text, (int(SCREEN_WIDTH * 0.07), int(SCREEN_HEIGHT * 0.9)))\n",
    "    screen.blit(success_text, (int(SCREEN_WIDTH * 0.7), int(SCREEN_HEIGHT * 0.9)))\n",
    "\n",
    "def draw_landing_pad(successful_landing):\n",
    "    \"\"\"Draw the landing pad on the screen.\"\"\"\n",
    "    pad_width = 100\n",
    "    pad_height = 15\n",
    "    pad_x = (SCREEN_WIDTH - pad_width) // 2\n",
    "    pad_y = SCREEN_HEIGHT - pad_height\n",
    "    color = GREEN if successful_landing else WHITE\n",
    "    pygame.draw.rect(screen, color, (pad_x, pad_y, pad_width, pad_height))\n",
    "\n",
    "def draw_lunar_ui(x, y, angle, episodeIndex, success_percentage, successful_landing):\n",
    "    \"\"\"Draw the entire user interface for the lunar lander simulation.\"\"\"\n",
    "    screen.blit(background_image, (0, 0))\n",
    "    draw_lander(x, y, angle)\n",
    "    draw_ui_text(episodeIndex, success_percentage)\n",
    "    draw_landing_pad(successful_landing)\n",
    "    pygame.display.flip()\n",
    "\n",
    "class DQN:\n",
    "    \"\"\"Deep Q-Network implementation for training the lunar lander.\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"Build the neural network model for the DQN.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update the target model with weights from the main model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store the experience in memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Select an action using the DQN or a random action.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \"\"\"Train the DQN using experiences from the memory.\"\"\"\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([experience[0][0] for experience in minibatch])\n",
    "        next_states = np.array([experience[3][0] for experience in minibatch])\n",
    "        targets = self.model.predict(states)\n",
    "        next_state_targets = self.target_model.predict(next_states)\n",
    "        \n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            if done:\n",
    "                targets[i][action] = reward\n",
    "            else:\n",
    "                targets[i][action] = reward + self.gamma * np.amax(next_state_targets[i])\n",
    "                \n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "agent = DQN(env)\n",
    "batch_size = 32\n",
    "num_episodes = 1000\n",
    "successful_landings = 0\n",
    "\n",
    "# Loop over all episodes for training\n",
    "for e in range(num_episodes):\n",
    "    # Initialize the reward for this episode\n",
    "    total_reward = 0\n",
    "    # Flag to check if the landing was successful in this episode\n",
    "    successful_landing = False\n",
    "    # Reset the environment for a new episode and get initial state\n",
    "    state, _ = env.reset()\n",
    "    # Reshape the state to match the expected input shape for the DQN model\n",
    "    state = np.reshape(state, [1, agent.state_size])\n",
    "    \n",
    "    # Loop for each time step in the episode\n",
    "    for time in range(500):\n",
    "        # Extract position and angle information from the state\n",
    "        x = state[0][0]\n",
    "        y = state[0][1]\n",
    "        angle = state[0][4]\n",
    "        # Calculate the success percentage so far\n",
    "        success_percentage = (successful_landings / (e + 1)) * 100\n",
    "        # Draw the UI with current state information\n",
    "        draw_lunar_ui(x, y, angle, e, success_percentage, successful_landing)\n",
    "        # Get an action from the DQN agent\n",
    "        action = agent.act(state)\n",
    "        # Perform the action in the environment\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "        # Modify the reward if the episode is done\n",
    "        reward = reward if not done else -10\n",
    "        # Reshape the next state to match the expected input shape for the DQN model\n",
    "        next_state = np.reshape(next_state, [1, agent.state_size])\n",
    "        # Store this experience in the agent's memory\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        # Set the current state for the next iteration\n",
    "        state = next_state\n",
    "\n",
    "        # Check for user exit request\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                exit()\n",
    "\n",
    "        # Check if episode is done\n",
    "        if done:\n",
    "            # If the total reward is above a threshold, consider it a successful landing\n",
    "            if total_reward > 50:\n",
    "                successful_landings += 1\n",
    "            # Update the target model of the DQN\n",
    "            agent.update_target_model()\n",
    "            break\n",
    "            \n",
    "        # Every 10 episodes, train the DQN with experiences from memory\n",
    "        if e % 10 == 0:\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "    # Check the landing conditions at the end of an episode\n",
    "    if total_reward > 50 and y < 0.1:\n",
    "        successful_landing = True\n",
    "\n",
    "    # Wait for a short duration before starting the next episode\n",
    "    pygame.time.wait(500)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "pygame.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
